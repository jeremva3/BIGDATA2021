{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400cc3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.2.0-bin-hadoop3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb57c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8581cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(\"local\",\"Test-app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77f3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa604eda",
   "metadata": {},
   "source": [
    "# Distancia de Minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62bfb0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4031242374328485\n"
     ]
    }
   ],
   "source": [
    "numeros=[(2,1),(3,1),(8,2)]\n",
    "p=2\n",
    "datos=sc.parallelize(numeros).zipWithIndex()\n",
    "\n",
    "mape=datos.flatMap(lambda x:[(x[1],i) for i in x[0]])\n",
    "reduce1=mape.reduceByKey(lambda x,y:(math.fabs(x-y))**p)\n",
    "mape1=reduce1.flatMap(lambda x:[(x[1]) for i in x]).distinct()\n",
    "reduce2=mape1.reduce(lambda x,y:(x+y))\n",
    "rdd=reduce2**(1/p)\n",
    "print(rdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e1be2",
   "metadata": {},
   "source": [
    "# Escalonamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c86f402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0, 0.3333333333333333]]\n"
     ]
    }
   ],
   "source": [
    "numeros2=[(9,6,7)]\n",
    "datos=sc.parallelize(numeros2)\n",
    "maximo=max(datos.first())\n",
    "minimo=min(datos.first())\n",
    "mape=datos.map(lambda x:[(i-minimo)/(maximo-minimo) for i in x])\n",
    "print(mape.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd543cb",
   "metadata": {},
   "source": [
    "# Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4596563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5773502691896258, 0.5773502691896258, 0.5773502691896258]\n"
     ]
    }
   ],
   "source": [
    "numeros2=[(1,1,1)]\n",
    "datos=sc.parallelize(numeros2)\n",
    "modulo=datos.flatMap(lambda x:((i**2) for i in x))\n",
    "reduce=modulo.reduce(lambda x,y:(x+y))\n",
    "mape=datos.flatMap(lambda x:[(i/(reduce)**0.5) for i in x])\n",
    "print(mape.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d25a3",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca4b703d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\n",
      "ID  -  WORD    -        TF   \n",
      "0 :  Profe :      0.3979400086720377\n",
      "0 :  disculpe :      0.3979400086720377\n",
      "0 :  la :      0.3979400086720377\n",
      "0 :  tardanza :      0.3979400086720377\n",
      "1 :  la :      0.5228787452803376\n",
      "1 :  UNSAAC :      0.22184874961635648\n",
      "1 :  es :      0.22184874961635648\n",
      "1 :  mejor :      0.22184874961635648\n",
      "1 :  universidad :      0.22184874961635648\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "#Creamos un corpus de 2 dimensiones\n",
    "corpus=[\"Profe disculpe la tardanza \",\"la UNSAAC es  la mejor universidad\"]\n",
    "#al corpus le damos un ID autoincrementable con la funcion zipWithIndex\n",
    "data=sc.parallelize(corpus).zipWithIndex()\n",
    "#Calculamos la cantidad de elementos dentro de cada oracion, para poder calcualar el TF\n",
    "#Ejemplo en \"messi messi messi ronaldo ronaldo balon\" hay 6 palabras\n",
    "map0=data.flatMap(lambda x: [((x[1],x[0]),1) for i in x[0].split()])\n",
    "reduce0=map0.reduceByKey(lambda x,y:x+y)\n",
    "#Guardamos  el ID de la oracion, el total de elementos de la oracion y la cantidad de apariciones de una palabra\n",
    "map1=reduce0.flatMap(lambda x: [((x[0][0],x[1],i),1) for i in x[0][1].split()])\n",
    "reduce1=map1.reduceByKey(lambda x,y:x+y)\n",
    "#Calculamos el TF\n",
    "tf=reduce1.map(lambda x: (x[0][2],(1+math.log(x[1]/x[0][1],10),x[0][0])))\n",
    "print(\"EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\")\n",
    "print(\"ID  -  WORD    -        TF   \")\n",
    "for x in tf.collect():\n",
    "    print(f'{x[1][1]} :  {x[0]} :      {x[1][0] }' ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64494cd8",
   "metadata": {},
   "source": [
    "# IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "170201a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\n",
      "ID  -  WORD    -       IDF    \n",
      " Profe :        0.47712125471966244\n",
      " disculpe :        0.47712125471966244\n",
      " la :        0.30102999566398114\n",
      " tardanza :        0.47712125471966244\n",
      " UNSAAC :        0.47712125471966244\n",
      " es :        0.47712125471966244\n",
      " mejor :        0.47712125471966244\n",
      " universidad :        0.47712125471966244\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "#Creamos un corpus de 2 dimensiones\n",
    "corpus=[\"Profe disculpe la tardanza \",\"la UNSAAC es  la mejor universidad\"]\n",
    "#al corpus le damos un ID autoincrementable con la funcion zipWithIndex\n",
    "data=sc.parallelize(corpus).zipWithIndex()\n",
    "#Calculamos la cantidad de elementos dentro de cada oracion, para poder calcualar el TF\n",
    "#Ejemplo en \"messi messi messi ronaldo ronaldo balon\" hay 6 palabras\n",
    "map0=data.flatMap(lambda x: [((x[1],x[0]),1) for i in x[0].split()])\n",
    "reduce0=map0.reduceByKey(lambda x,y:x+y)\n",
    "#Guardamos  el ID de la oracion, el total de elementos de la oracion y la cantidad de apariciones de una palabra\n",
    "map1=reduce0.flatMap(lambda x: [((x[0][0],x[1],i),1) for i in x[0][1].split()])\n",
    "reduce1=map1.reduceByKey(lambda x,y:x+y)\n",
    "#Guardamos la palabra si es que aparece como minimo en una oracion acompa√±ado de un 1\n",
    "#para luego poder determinar en cuantas oraciones aparece esta palabra\n",
    "map2=reduce1.flatMap(lambda x: [(x[0][2],1)] )\n",
    "reduce2=map2.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "#Calculamos el total de oraciones del corpus\n",
    "total=data.count()\n",
    "\n",
    "#Calculamos el idf\n",
    "idf=reduce2.map(lambda x: (x[0],math.log(1+total/x[1],10)))\n",
    "\n",
    "#MOSTRAMOS LOS RESULTADOS\n",
    "print(\"EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\")\n",
    "print(\"ID  -  WORD    -       IDF    \")\n",
    "for x in idf.collect():\n",
    "    print(f' {x[0]} :        {x[1] }' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e56b0",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\n",
      "ID  -  WORD    -      TF        -         IDF        -        TF-IDF\n",
      "1 :  disculpe : (0.3979400086720377, 0.47712125471966244, 0.18986563624075598)\n",
      "1 :  tardanza : (0.3979400086720377, 0.47712125471966244, 0.18986563624075598)\n",
      "2 :  mejor : (0.22184874961635648, 0.47712125471966244, 0.10584875377494424)\n",
      "2 :  universidad : (0.22184874961635648, 0.47712125471966244, 0.10584875377494424)\n",
      "1 :  Profe : (0.3979400086720377, 0.47712125471966244, 0.18986563624075598)\n",
      "1 :  la : (0.3979400086720377, 0.30102999566398114, 0.11979187908506814)\n",
      "2 :  la : (0.5228787452803376, 0.30102999566398114, 0.15740218642452794)\n",
      "2 :  UNSAAC : (0.22184874961635648, 0.47712125471966244, 0.10584875377494424)\n",
      "2 :  es : (0.22184874961635648, 0.47712125471966244, 0.10584875377494424)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "#Creamos un corpus de 2 dimensiones\n",
    "corpus=[\"Profe disculpe la tardanza \",\"la UNSAAC es  la mejor universidad\"]\n",
    "#al corpus le damos un ID autoincrementable con la funcion zipWithIndex\n",
    "data=sc.parallelize(corpus).zipWithIndex()\n",
    "#Calculamos la cantidad de elementos dentro de cada oracion, para poder calcualar el TF\n",
    "#Ejemplo en \"messi messi messi ronaldo ronaldo balon\" hay 6 palabras\n",
    "map0=data.flatMap(lambda x: [((x[1],x[0]),1) for i in x[0].split()])\n",
    "reduce0=map0.reduceByKey(lambda x,y:x+y)\n",
    "#Guardamos  el ID de la oracion, el total de elementos de la oracion y la cantidad de apariciones de una palabra\n",
    "map1=reduce0.flatMap(lambda x: [((x[0][0],x[1],i),1) for i in x[0][1].split()])\n",
    "reduce1=map1.reduceByKey(lambda x,y:x+y)\n",
    "#Calculamos el TF\n",
    "tf=reduce1.map(lambda x: (x[0][2],(1+math.log(x[1]/x[0][1],10),x[0][0])))\n",
    "\n",
    "#Guardamos la palabra si es que aparece como minimo en una oracion acompa√±ado de un 1\n",
    "#para luego poder determinar en cuantas oraciones aparece esta palabra\n",
    "map2=reduce1.flatMap(lambda x: [(x[0][2],1)] )\n",
    "reduce2=map2.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "#Calculamos el total de oraciones del corpus\n",
    "total=data.count()\n",
    "\n",
    "#Calculamos el idf\n",
    "idf=reduce2.map(lambda x: (x[0],math.log(1+total/x[1],10)))\n",
    "\n",
    "#Hacemos un join\n",
    "rdd=tf.join(idf)\n",
    "\n",
    "#Por ultimos guardamos dentro de \"rdd\" los elementos obtenidos\n",
    "# con el siguiente esquema\n",
    "#            Identificador       ,        Word ,    Tf   ,   IDF ,   TF*IDF \n",
    "rdd=rdd.map(lambda x: (x[1][0][1],x[0],(x[1][0][0],x[1][1],x[1][0][0]*x[1][1])))\n",
    "\n",
    "#MOSTRAMOS LOS RESULTADOS\n",
    "print(\"EL ID ES EL IDENTIFICADOR, QUE NOS INDICA EN QUE NUMERO DE ORACION HA APARECIDO ESTA PALABRA\")\n",
    "print(\"ID  -  WORD    -      TF        -         IDF        -        TF-IDF\")\n",
    "for x in rdd.collect():\n",
    "    print(f'{x[0]+1} :  {x[1]} : {x[2] }' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005f314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
